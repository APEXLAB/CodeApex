# 

![paper](/Users/fulingyue/Desktop/DataMining/LLM4EDU/CodeApex/figures/paper.png)

CodeApex is a bilingual  programming evaluation benchmark for Large Language Models. It consists two basic programming tasks: programming comprehension and code generation. **Programming Comprehension Test** consists of 250 multiple choice quesitions, including conceptual understanding, commonsense reasoning, and multi-hop reasoning three question categories. **Code generation Task** consists of 477  C++ based algorithm problems, covering common algorithm knowledge points like binary search, depth-firsts-search and so on. In the future, CodeApex will publish other code-related functional tests, such as code correction.

<img src="/Users/fulingyue/Desktop/DataMining/LLM4EDU/CodeApex/figures/intro.png" alt="Overview diagram of CodeApex benchamark." style="zoom:40%;" />



### News

- [2023.09.05] CodeApex is published on arXiv and [Github](https://github.com/APEXLAB/CodeApex.git). Benchmark [website]((https://apex.sjtu.edu.cn/codeapex/)) is open for commission now.



#### Table of Contents





Open our [benchmark website](https://apex.sjtu.edu.cn/codeapex/) for evaluation.

## 1. Register
Click register button. Input your username (unique for everyone), password, confirmed password, and email, and then click "Send Verifiction Code" button and wait for email verifiction code (valid in 5 minute). Input code and register.


## 2. Login
After registering, you should click login button to sign in.


## 3. Leaderboard
The leaderboard page is a leaderboard, which contains a Programming Comprehension Leaderboard in Chinese and a Programming Comprehension Leaderboard in English. In these two leaderboards, CU score means Conceptual Understanding scores, CR score for Commonsense Reasoning, and MCR stands for Multi-hop Reasoning. The Code Generatation Leaderboards are also completed.


## 4. Submit
#### 1) Answer Format
Users should be responsible for the correctness and compliance of their inputs. The format of answer generated by LLM is json, and the json file is divided into three dictionaries in order, representing the answers for CU, CR, and MCR, with each dictionary's answers sorted by ID within the dictionary. We provide a example.json.

Your input should be a npy file containing your answer to the testcases, and run the deal_answer.py to generate the json file for evaluation.
```
python deal_answer.py
```
#### 2) How to submit
You should login and click submit page. First click "choose file" button, and then input the name, author and description of your model name. Then click upload button, then click process butto, and the score of your model will appear. You will have a daily limit of 5 submissions.

Due to time constraints, there are some issues with the website. Please bear with us.

